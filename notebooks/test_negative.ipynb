{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "a61fc264cbb8ce87a9adf5bfb7751dff026c03132aceb8abaf97df9013f35094"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../fcstapi_fcsttble_202103100635.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.trsdate = pd.to_datetime(df.trsdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.query(\"trsdate < '2020-10-01'\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 737 entries, 0 to 1120\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype         \n---  ------    --------------  -----         \n 0   id        737 non-null    int64         \n 1   clientid  737 non-null    object        \n 2   pid       737 non-null    object        \n 3   pnme      737 non-null    object        \n 4   value     737 non-null    float64       \n 5   trsdate   737 non-null    datetime64[ns]\n 6   value2    737 non-null    float64       \ndtypes: datetime64[ns](1), float64(2), int64(1), object(3)\nmemory usage: 46.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8: noqa: E501\n",
    "\n",
    "import numpy as np\n",
    "from numpy.core.fromnumeric import mean\n",
    "import pandas as pd\n",
    "from pandas.core.indexes.api import get_objs_combined_axis\n",
    "import pmdarima\n",
    "from datetime import datetime\n",
    "import typing\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error # upgrade to the the latest scikit-learn pip install -U scikit-learn\n",
    "import random\n",
    "\n",
    "## Changes to be made\n",
    "## 1 would like to add an argument \n",
    "\n",
    "class TSFM(object):\n",
    "    def __init__(self,\n",
    "                 df: pd.core.frame.DataFrame,\n",
    "                 n_pred_period: int,\n",
    "                 date_variable: typing.Union[int, str],\n",
    "                 value_variable: typing.Union[int, str],\n",
    "                 target_variable: str,\n",
    "                 stop_date: str,         # stop date of train set, to split df to train and test sets\n",
    "                 section_list: list = None,\n",
    "                 cycle_length: int = 12,\n",
    "                 alpha: float = 0.05,\n",
    "                 stepwise: bool = True,\n",
    "                 start_order: tuple = (0, 1, 0),\n",
    "                 max_order: tuple = (4, 2, 5),\n",
    "                 start_seasonal_order: tuple = (0, 1, 0),\n",
    "                 max_seasonal_order: tuple = (2, 2, 4)):\n",
    "        self.target_variable = target_variable\n",
    "        # self.n_pred_period = n_pred_period + abs((datetime.strptime(df[date_variable].to_numpy()[-1])  - datetime.strptime(stop_date, \"%Y-%m-%d\")).days)\n",
    "        self.n_pred_period = n_pred_period\n",
    "        self.stop_date = stop_date\n",
    "        self.cycle_length = cycle_length\n",
    "        self.alpha = alpha\n",
    "        self.stepwise = stepwise\n",
    "        self.start_order = start_order\n",
    "        self.max_order = max_order\n",
    "        self.start_seasonal_order = start_seasonal_order\n",
    "        self.max_seasonal_order = max_seasonal_order\n",
    "\n",
    "        self.is_log_transformed_dict = dict()\n",
    "        self.pred_dict = dict()\n",
    "        self.pred_ic_dict = dict()\n",
    "        self.adjusted_pred_dict = dict()\n",
    "        self.adjusted_pred_ic_dict = dict()\n",
    "\n",
    "        # train and test df must have date as index, and 2 columns: sections(e.g. territory) and values(e.g. order_volume)\n",
    "        if type(date_variable) is int:\n",
    "            date_variable = df.columns[date_variable]\n",
    "        if type(value_variable) is int:\n",
    "            value_variable = df.columns[value_variable]\n",
    "        # Select relevant columns for train and test df, create empty pred df\n",
    "        self.columns = [date_variable, target_variable, value_variable]\n",
    "        df[self.columns[0]] = pd.to_datetime(df[self.columns[0]])\n",
    "        \n",
    "\n",
    "        self.df = df.copy()\n",
    "\n",
    "        # keys: sections(territories), value: list(train, test, pred), for easy storing and fetching data\n",
    "        self.df_dict = dict()\n",
    "        self.model_dict = dict()\n",
    "        self.adjusted_model_dict = dict()\n",
    "\n",
    "        # Iterate through the unique sections\n",
    "        self.section_list = section_list\n",
    "        if self.section_list is None:\n",
    "            self.section_list = df[target_variable].unique()\n",
    "        for section in self.section_list:\n",
    "            self.is_log_transformed_dict[section] = False\n",
    "            models = self.__train_models(section = section, is_log_transformed=False)\n",
    "            if self.__have_negative_prediction(models=models):\n",
    "                print(\"Negative prediction detected in\", section)\n",
    "                self.is_log_transformed_dict[section] = True\n",
    "                models = self.__train_models(section = section, is_log_transformed=True)\n",
    "            (arima_model, adjusted_arima_model) = models\n",
    "            self.model_dict[section] = arima_model\n",
    "            self.adjusted_model_dict[section] = adjusted_arima_model\n",
    "            self.pred_dict[section], self.pred_ic_dict[section] = self.__get_pred_data(section=section, return_conf_int=True, is_adjusted=False)\n",
    "            self.adjusted_pred_dict[section], self.adjusted_pred_ic_dict[section] = self.__get_pred_data(section=section, return_conf_int=True, is_adjusted=True)\n",
    "\n",
    "            # print(\"Inspecting\", section, \"...\")\n",
    "            # temp_actual_df = self.get_actual_data(section=section, is_adjusted=False)\n",
    "            # if temp_actual_df.shape[0] >= 2 * cycle_length:\n",
    "            #     # train actual data\n",
    "            #     print(\"Training\", temp_actual_df.shape[0], \"actual records ...\")\n",
    "            #     arima_model = pmdarima.auto_arima(temp_actual_df[temp_actual_df.columns[-1]],\n",
    "            #                                     start_p=start_p, start_P=start_P,\n",
    "            #                                     start_q=start_q, start_Q=start_Q,\n",
    "            #                                     d=start_d, D=start_D,\n",
    "            #                                     max_p=max_p, max_P=max_P,\n",
    "            #                                     max_d=max_d, max_D=max_D,\n",
    "            #                                     max_q=max_q, max_Q=max_Q,\n",
    "            #                                     trace=True, m=cycle_length, stepwise=stepwise)\n",
    "            #     self.model_dict[section] = arima_model\n",
    "\n",
    "            #     temp_adjusted_actual_df = self.get_actual_data(section=section, is_adjusted=True)\n",
    "            #     # train adjusted actual data\n",
    "            #     print(\"Training\", temp_adjusted_actual_df.shape[0], \"adjusted actual records ...\")\n",
    "            #     arima_model = pmdarima.auto_arima(temp_adjusted_actual_df[temp_adjusted_actual_df.columns[-1]],\n",
    "            #                                     start_p=start_p, start_P=start_P,\n",
    "            #                                     start_q=start_q, start_Q=start_Q,\n",
    "            #                                     d=start_d, D=start_D,\n",
    "            #                                     max_p=max_p, max_P=max_P,\n",
    "            #                                     max_d=max_d, max_D=max_D,\n",
    "            #                                     max_q=max_q, max_Q=max_Q,\n",
    "            #                                     trace=True, m=cycle_length, stepwise=stepwise)\n",
    "            #     self.adjusted_model_dict[section] = arima_model\n",
    "            # else:\n",
    "            #     print(\"Number of data points in Section\", section, \"is too small (\" + str(\n",
    "            #         temp_actual_df.shape[0]) + \". Must be at least twice the declared cycle length.\")\n",
    "\n",
    "    # DF Getters--------------------------------------------------------------\n",
    "    def get_actual_data(self, section: str, is_adjusted: bool, is_log_transformed: bool = False) -> pd.core.frame.DataFrame:\n",
    "        agg_df = self.df[self.columns].groupby(self.columns[0:2], as_index=False).sum().copy()\n",
    "        agg_df = agg_df.query(self.columns[1] + \"==\" + \"'\" + section + \"'\")[[self.columns[0], self.columns[2]]]\n",
    "        agg_df.set_index(self.columns[0], inplace=True)\n",
    "        agg_df = TSFM.to_monthly(agg_df)\n",
    "        if is_adjusted:\n",
    "            agg_df = self.anomaly_filter(agg_df, alpha = self.alpha)\n",
    "        if is_log_transformed:\n",
    "            agg_df = TSFM.log_transform(agg_df)\n",
    "        return agg_df\n",
    "\n",
    "    def get_train_data(self, section: str):  ## added stop date\n",
    "        actual_df = self.get_actual_data(section, is_adjusted=False)\n",
    "        return actual_df.iloc[lambda x: x.index <= self.stop_date].copy()\n",
    "\n",
    "    def get_test_data(self, section: str, ):\n",
    "        actual_df = self.get_actual_data(section)\n",
    "        return actual_df.iloc[lambda x: x.index > self.stop_date].copy()\n",
    "\n",
    "    def get_pred_data(self, section: str, return_conf_int: bool = False, is_adjusted: bool = True):\n",
    "        pred, ic = None, None\n",
    "        if is_adjusted:\n",
    "            pred, ic = self.adjusted_pred_dict[section], self.adjusted_pred_ic_dict[section]\n",
    "        else:\n",
    "            pred, ic = self.pred_dict[section], self.pred_ic_dict[section]\n",
    "        if return_conf_int:\n",
    "            return pred, ic\n",
    "        return pred\n",
    "\n",
    "    def __get_pred_data(self, section: str, return_conf_int: bool = False, is_adjusted: bool = True):\n",
    "        actual_df = self.get_actual_data(section, False)\n",
    "        model = self.get_model(section=section, is_adjusted=is_adjusted)\n",
    "        if model is None:\n",
    "            print(\"Model of\", section, \"section was not initiated. Might due to insufficient training data.\")\n",
    "            return None\n",
    "        print(model.predict(self.n_pred_period, return_conf_int=return_conf_int))\n",
    "        pred, conf_int = model.predict(self.n_pred_period, return_conf_int=True)\n",
    "\n",
    "        temp_pred_df = pd.DataFrame(\n",
    "            data={\n",
    "                self.columns[0]: pd.date_range(max(actual_df.index),freq='MS',periods=self.n_pred_period+1)[1:],\n",
    "                self.columns[1]: [section for x in range(len(pred))],\n",
    "                self.columns[-1]: pred})  # Use numbers inplace of future dates for now)\n",
    "        temp_pred_df = temp_pred_df[[self.columns[0], self.columns[2]]]\n",
    "        temp_pred_df.set_index(self.columns[0], inplace=True)\n",
    "        if self.is_log_transformed_dict[section]:\n",
    "            temp_pred_df = self.exp_transform(temp_pred_df)\n",
    "        if return_conf_int:\n",
    "            return temp_pred_df.copy(), conf_int\n",
    "        return temp_pred_df.copy()\n",
    "\n",
    "    def get_pred_df(self):\n",
    "        '''\n",
    "        function to be called in the backend to get pred data from all sections\n",
    "        '''\n",
    "        return_df = pd.DataFrame(columns=self.columns)\n",
    "        for section in self.section_list:\n",
    "            actual_pred = self.get_pred_data(section, is_adjusted=False)\n",
    "            adjusted_actual_pred = self.get_pred_data(section, is_adjusted=True)\n",
    "            pred = pd.DataFrame(data={actual_pred.columns[0]: actual_pred[actual_pred.columns[0]].to_numpy(), \"value2\": adjusted_actual_pred[adjusted_actual_pred.columns[0]].to_numpy()},\n",
    "                                index = actual_pred.index)\n",
    "            pred.reset_index(inplace=True)\n",
    "            pred[self.target_variable] = [section for x in range(pred.shape[0])]\n",
    "            return_df = return_df.append(pred, ignore_index=True)\n",
    "        return_df.sort_values(by=[self.columns[1], self.columns[0]], inplace=True, ignore_index=True)\n",
    "        return return_df\n",
    "\n",
    "    # Model Getters----------------------------------------------------------\n",
    "    def get_model(self, section: str, is_adjusted: bool = True):\n",
    "        if is_adjusted:\n",
    "            return self.adjusted_model_dict[section]\n",
    "        return self.model_dict[section]\n",
    "    \n",
    "    # Plot Function-----------------------------------------------------------\n",
    "    def plot(self, section: str):\n",
    "        actual = self.get_actual_data(section, is_adjusted=False)\n",
    "        adjusted_actual, conf_int_df = self.anomaly_filter(actual, return_conf_int=True, alpha = self.alpha)\n",
    "        # pred, ci = self.get_pred_data(section, return_conf_int=True)\n",
    "\n",
    "        actual_pred = self.get_pred_data(section, is_adjusted=False)\n",
    "        adjusted_actual_pred = self.get_pred_data(section, is_adjusted=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(14,6))\n",
    "        ax.plot(actual.index, actual[actual.columns[0]].to_numpy(),label=\"Actual\")   #Actuals This should come from original DS (all actuals)\n",
    "        ax.plot(adjusted_actual.index, adjusted_actual[adjusted_actual.columns[0]].to_numpy(),'-g', label=\"Adjusted Actual\")   \n",
    "        # ax.plot(pred.index, pred[pred.columns[0]], '-r',alpha=0.75,label=\"Forecast\")  ## Pred\n",
    "        ax.fill_between(conf_int_df.index, conf_int_df.iloc[:, 0], conf_int_df.iloc[:, 1],alpha=0.3, color='b')  ## Conf intervals\n",
    "        \n",
    "        ax.plot(actual_pred.index, actual_pred.iloc[:, 0], '--b',alpha=0.75,label=\"Actual Forecast\")\n",
    "        ax.plot(adjusted_actual_pred.index, adjusted_actual_pred.iloc[:, 0], '--g',alpha=0.75,label=\"Adjusted Actual Forecast\")\n",
    "        plt.title('Forecast Model')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Forecast Accurary')\n",
    "\n",
    "        ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "        ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def anomaly_filter(self,\n",
    "                       df: pd.core.frame.DataFrame,\n",
    "                       return_conf_int: bool = False, \n",
    "                       n_rolling_period: int = 12,\n",
    "                       alpha: float = 0.05):\n",
    "        print(\"Applying anomaly filter...\")\n",
    "        df = df.copy()\n",
    "        train = df.iloc[lambda x: x.index <= self.stop_date]\n",
    "        stop_date = max(train.index)\n",
    "        returning_ic_list = list()\n",
    "        (start_p, start_d, start_q) = self.start_order\n",
    "        (max_p, max_d, max_q) = self.max_order\n",
    "        (start_P, start_D, start_Q) = self.start_seasonal_order\n",
    "        (max_P, max_D, max_Q) = self.max_seasonal_order\n",
    "        for i in range(train.shape[0], df.shape[0], n_rolling_period):\n",
    "            arima_model = pmdarima.auto_arima(train[train.columns[0]],\n",
    "                                                start_p=start_p, start_P=start_P,\n",
    "                                                start_q=start_q, start_Q=start_Q,\n",
    "                                                d=start_d, D=start_D,\n",
    "                                                max_p=max_p, max_P=max_P,\n",
    "                                                max_d=max_d, max_D=max_D,\n",
    "                                                max_q=max_q, max_Q=max_Q,\n",
    "                                                trace=True, m=self.cycle_length, stepwise=self.stepwise)\n",
    "            temp_actual_df = df.iloc[i:min(i+n_rolling_period, df.shape[0]), :].copy()\n",
    "            temp_pred, ic_list = arima_model.predict(n_rolling_period, return_conf_int=True, alpha=alpha)\n",
    "            for j in range(temp_actual_df.shape[0]):\n",
    "                temp_actual = temp_actual_df.iloc[j, 0]\n",
    "                ic = ic_list[j]\n",
    "                if temp_actual < ic[0] or temp_actual > ic[1]:\n",
    "                    temp_actual_df.iloc[j, 0] = temp_pred[j]\n",
    "            train = train.append(temp_actual_df)\n",
    "            returning_ic_list = returning_ic_list + ic_list.tolist()\n",
    "        train[train.columns[0]] = train[train.columns[0]].astype('float')\n",
    "        train = train.asfreq('MS')\n",
    "        if return_conf_int:\n",
    "            returning_ic_list = np.array(returning_ic_list)\n",
    "            ic_df = pd.DataFrame(\n",
    "                data={\n",
    "                    'lower': returning_ic_list[:, 0],\n",
    "                    'upper': returning_ic_list[:, 1],\n",
    "                },\n",
    "                index=pd.date_range(stop_date,freq='MS',periods=returning_ic_list.shape[0]+1)[1:]\n",
    "            )\n",
    "            return train.copy(), ic_df\n",
    "        return train.copy()\n",
    "    \n",
    "    def cross_validate(self,section: str, is_adjusted: bool):\n",
    "        actual_df = self.get_actual_data(section, is_adjusted)\n",
    "        actual_arr = actual_df[actual_df.columns[0]].to_numpy(dtype='float')\n",
    "        trained_model = self.get_model(section, is_adjusted)\n",
    "        order = trained_model.order\n",
    "        seasonal_order = trained_model.seasonal_order\n",
    "\n",
    "        model = pmdarima.ARIMA(order = order, seasonal_order=seasonal_order)\n",
    "        MAE = list()\n",
    "        MAPE = list()\n",
    "        tscv = TimeSeriesSplit(10)\n",
    "        for train_index, test_index in tscv.split(X=actual_df.index):\n",
    "            if len(train_index) >= 24:\n",
    "                print(\"train_index = {}\".format(train_index))\n",
    "                print(\"test_index = {}\".format(test_index))\n",
    "                train = actual_arr[train_index]\n",
    "                test = actual_arr[test_index]\n",
    "                print(\"train = {}\".format(train))\n",
    "                print(\"test = {}\".format(test))\n",
    "                model.fit(train)\n",
    "                pred = model.predict(len(test))\n",
    "                mae = mean_absolute_error(test, pred)\n",
    "                mape = mean_absolute_percentage_error(test, pred)\n",
    "                MAE.append(mae)\n",
    "                MAPE.append(mape)\n",
    "                \n",
    "                print(\"MAE = {}\".format(mae))\n",
    "                print(\"MAPE = {}\".format(mape))\n",
    "                print(\"-\"*50)\n",
    "            else:\n",
    "                pass\n",
    "        print(\"MAE = {}\".format(MAE))\n",
    "        print(\"MAPE = {}\".format(MAPE))\n",
    "        print(\"Average MAE = {}\".format(np.mean(MAE)))\n",
    "        print(\"Average MAPE = {}\".format(np.mean(MAPE)))\n",
    "\n",
    "    def __train_models(self, section: str, is_log_transformed: bool) -> tuple:\n",
    "        (start_p, start_d, start_q) = self.start_order\n",
    "        (max_p, max_d, max_q) = self.max_order\n",
    "        (start_P, start_D, start_Q) = self.start_seasonal_order\n",
    "        (max_P, max_D, max_Q) = self.max_seasonal_order\n",
    "    \n",
    "        print(\"Inspecting\", section, \"...\")\n",
    "        temp_actual_df = self.get_actual_data(section=section, is_adjusted=False, is_log_transformed=is_log_transformed)\n",
    "        if temp_actual_df.shape[0] >= 2 * self.cycle_length:\n",
    "            # train actual data\n",
    "            print(\"Training\", temp_actual_df.shape[0], \"actual records ...\")\n",
    "            arima_model = pmdarima.auto_arima(temp_actual_df[temp_actual_df.columns[-1]],\n",
    "                                            start_p=start_p, start_P=start_P,\n",
    "                                            start_q=start_q, start_Q=start_Q,\n",
    "                                            d=start_d, D=start_D,\n",
    "                                            max_p=max_p, max_P=max_P,\n",
    "                                            max_d=max_d, max_D=max_D,\n",
    "                                            max_q=max_q, max_Q=max_Q,\n",
    "                                            trace=True, m=self.cycle_length, stepwise=self.stepwise)\n",
    "\n",
    "            temp_adjusted_actual_df = self.get_actual_data(section=section, is_adjusted=True, is_log_transformed=is_log_transformed)\n",
    "            # train adjusted actual data\n",
    "            print(\"Training\", temp_adjusted_actual_df.shape[0], \"adjusted actual records ...\")\n",
    "            adjusted_arima_model = pmdarima.auto_arima(temp_adjusted_actual_df[temp_adjusted_actual_df.columns[-1]],\n",
    "                                            start_p=start_p, start_P=start_P,\n",
    "                                            start_q=start_q, start_Q=start_Q,\n",
    "                                            d=start_d, D=start_D,\n",
    "                                            max_p=max_p, max_P=max_P,\n",
    "                                            max_d=max_d, max_D=max_D,\n",
    "                                            max_q=max_q, max_Q=max_Q,\n",
    "                                            trace=True, m=self.cycle_length, stepwise=self.stepwise)\n",
    "            return arima_model, adjusted_arima_model\n",
    "        print(\"Number of data points in Section\", section, \"is too small (\" + str(\n",
    "            temp_actual_df.shape[0]) + \". Must be at least twice the declared cycle length.\")\n",
    "        return None, None\n",
    "\n",
    "    def __have_negative_prediction(self, models: tuple) -> bool:\n",
    "        (model, adjusted_model) = models\n",
    "        if model is None or adjusted_model is None:\n",
    "            return False\n",
    "        pred = model.predict(self.n_pred_period)\n",
    "        adjusted_pred = adjusted_model.predict(self.n_pred_period)\n",
    "        if min(min(pred), min(adjusted_pred)) < 0:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def log_transform(df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n",
    "        '''\n",
    "        df has 1 value column and has dates as indeces\n",
    "        '''\n",
    "        value_arr = df.loc[:, df.columns[0]].values.copy()\n",
    "        value_arr[value_arr < 1] = 1 # avoid negative results from transformation\n",
    "        value_arr = np.log(value_arr)\n",
    "        return  pd.DataFrame(index=df.index, data={df.columns[0]: value_arr})\n",
    "    \n",
    "    @staticmethod\n",
    "    def exp_transform(df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n",
    "        '''\n",
    "        df has 1 value column and has dates as indeces\n",
    "        '''\n",
    "        value_arr = df.loc[:, df.columns[0]].values.copy()\n",
    "        value_arr = np.exp(value_arr)\n",
    "        return  pd.DataFrame(index=df.index, data={df.columns[0]: value_arr})\n",
    "\n",
    "    @classmethod\n",
    "    def to_monthly(cls, df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n",
    "        return df.resample('MS').sum()\n",
    "\n",
    "    def foo():\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Inspecting Accessories ...\n",
      "Training 45 actual records ...\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,1,0)(0,1,0)[12]             : AIC=615.359, Time=0.06 sec\n",
      " ARIMA(1,1,0)(1,1,0)[12]             : AIC=608.112, Time=0.62 sec\n",
      " ARIMA(0,1,1)(0,1,1)[12]             : AIC=602.336, Time=0.95 sec\n",
      " ARIMA(0,1,1)(0,1,0)[12]             : AIC=607.683, Time=0.19 sec\n",
      " ARIMA(0,1,1)(1,1,1)[12]             : AIC=604.315, Time=1.26 sec\n",
      " ARIMA(0,1,1)(0,1,2)[12]             : AIC=inf, Time=0.94 sec\n",
      " ARIMA(0,1,1)(1,1,0)[12]             : AIC=603.892, Time=0.77 sec\n",
      " ARIMA(0,1,1)(1,1,2)[12]             : AIC=inf, Time=2.33 sec\n",
      " ARIMA(0,1,0)(0,1,1)[12]             : AIC=607.948, Time=0.37 sec\n",
      " ARIMA(1,1,1)(0,1,1)[12]             : AIC=604.335, Time=1.76 sec\n",
      " ARIMA(0,1,2)(0,1,1)[12]             : AIC=604.335, Time=0.72 sec\n",
      " ARIMA(1,1,0)(0,1,1)[12]             : AIC=604.954, Time=0.42 sec\n",
      " ARIMA(1,1,2)(0,1,1)[12]             : AIC=605.190, Time=1.21 sec\n",
      " ARIMA(0,1,1)(0,1,1)[12] intercept   : AIC=inf, Time=1.25 sec\n",
      "\n",
      "Best model:  ARIMA(0,1,1)(0,1,1)[12]          \n",
      "Total fit time: 12.893 seconds\n",
      "Applying anomaly filter...\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,1,0)(0,1,0)[12]             : AIC=381.678, Time=0.03 sec\n",
      " ARIMA(1,1,0)(1,1,0)[12]             : AIC=379.654, Time=0.19 sec\n",
      " ARIMA(0,1,1)(0,1,1)[12]             : AIC=inf, Time=0.34 sec\n",
      " ARIMA(1,1,0)(0,1,0)[12]             : AIC=377.660, Time=0.03 sec\n",
      " ARIMA(1,1,0)(0,1,1)[12]             : AIC=379.652, Time=0.10 sec\n",
      " ARIMA(1,1,0)(1,1,1)[12]             : AIC=381.531, Time=0.40 sec\n",
      " ARIMA(2,1,0)(0,1,0)[12]             : AIC=372.075, Time=0.06 sec\n",
      " ARIMA(2,1,0)(1,1,0)[12]             : AIC=373.961, Time=0.30 sec\n",
      " ARIMA(2,1,0)(0,1,1)[12]             : AIC=373.953, Time=0.29 sec\n",
      " ARIMA(2,1,0)(1,1,1)[12]             : AIC=375.942, Time=0.42 sec\n",
      " ARIMA(3,1,0)(0,1,0)[12]             : AIC=374.153, Time=0.09 sec\n",
      " ARIMA(2,1,1)(0,1,0)[12]             : AIC=374.296, Time=0.14 sec\n",
      " ARIMA(1,1,1)(0,1,0)[12]             : AIC=inf, Time=0.19 sec\n",
      " ARIMA(3,1,1)(0,1,0)[12]             : AIC=inf, Time=0.36 sec\n",
      " ARIMA(2,1,0)(0,1,0)[12] intercept   : AIC=374.104, Time=0.08 sec\n",
      "\n",
      "Best model:  ARIMA(2,1,0)(0,1,0)[12]          \n",
      "Total fit time: 3.027 seconds\n",
      "Training 45 adjusted actual records ...\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,1,0)(0,1,0)[12]             : AIC=619.307, Time=0.03 sec\n",
      " ARIMA(1,1,0)(1,1,0)[12]             : AIC=614.884, Time=0.13 sec\n",
      " ARIMA(0,1,1)(0,1,1)[12]             : AIC=603.220, Time=0.68 sec\n",
      " ARIMA(0,1,1)(0,1,0)[12]             : AIC=602.529, Time=0.18 sec\n",
      " ARIMA(0,1,1)(1,1,0)[12]             : AIC=603.441, Time=0.74 sec\n",
      " ARIMA(0,1,1)(1,1,1)[12]             : AIC=inf, Time=1.30 sec\n",
      " ARIMA(1,1,1)(0,1,0)[12]             : AIC=604.310, Time=0.25 sec\n",
      " ARIMA(0,1,2)(0,1,0)[12]             : AIC=604.132, Time=0.47 sec\n",
      " ARIMA(1,1,0)(0,1,0)[12]             : AIC=614.225, Time=0.04 sec\n",
      " ARIMA(1,1,2)(0,1,0)[12]             : AIC=603.049, Time=0.63 sec\n",
      " ARIMA(0,1,1)(0,1,0)[12] intercept   : AIC=inf, Time=0.30 sec\n",
      "\n",
      "Best model:  ARIMA(0,1,1)(0,1,0)[12]          \n",
      "Total fit time: 4.781 seconds\n",
      "(array([12945.65151045, 14560.49729258, 16814.5287086 , 12575.40052195,\n",
      "       12802.43684493, 13877.31836611, 12735.15306818, 14069.22689948,\n",
      "       14343.74199239, 17035.69334886, 16983.62079125, 18072.11649873,\n",
      "       16854.40212076, 18469.24790289, 20723.27931891, 16484.15113225,\n",
      "       16711.18745524, 17786.06897642, 16643.90367849, 17977.97750979,\n",
      "       18252.4926027 , 20944.44395917, 20892.37140156, 21980.86710904]), array([[ 7848.13377611, 18043.1692448 ],\n",
      "       [ 8908.97546321, 20212.01912195],\n",
      "       [10665.63564154, 22963.42177566],\n",
      "       [ 5967.72495804, 19183.07608585],\n",
      "       [ 5768.40571257, 19836.46797729],\n",
      "       [ 6440.90287414, 21313.73385808],\n",
      "       [ 4916.84209799, 20553.46403838],\n",
      "       [ 5886.69674892, 22251.75705004],\n",
      "       [ 5812.36691023, 22875.11707456],\n",
      "       [ 8168.8843517 , 25902.50234601],\n",
      "       [ 7792.96650141, 26174.27508109],\n",
      "       [ 8567.22791676, 27577.00508069],\n",
      "       [ 6471.35532247, 27237.44891905],\n",
      "       [ 7583.08671863, 29355.40908714],\n",
      "       [ 9353.64088794, 32092.91774988],\n",
      "       [ 4664.17926982, 28304.12299468],\n",
      "       [ 4463.1311942 , 28959.24371628],\n",
      "       [ 5123.91285743, 30448.22509541],\n",
      "       [ 3580.55144444, 29707.25591254],\n",
      "       [ 4525.25702049, 31430.69799909],\n",
      "       [ 4421.21499329, 32083.77021212],\n",
      "       [ 6744.4359065 , 35144.45201183],\n",
      "       [ 6332.41248445, 35452.33031866],\n",
      "       [ 7068.41683301, 36893.31738507]]))\n",
      "(array([ 6097.65549179, 10393.12989179, 10502.17279179,  6819.75809179,\n",
      "        8323.03809179, 10061.95989179,  6562.84389179,  9746.30909179,\n",
      "       11757.97969179,  4110.81870944,  7759.00132069, 16910.00829179,\n",
      "        8878.18458358, 13173.65898358, 13282.70188358,  9600.28718358,\n",
      "       11103.56718358, 12842.48898358,  9343.37298358, 12526.83818358,\n",
      "       14538.50878358,  6891.34780123, 10539.53041248, 19690.53738358]), array([[  674.23376381, 11521.07721976],\n",
      "       [ 4930.07457972, 15856.18520385],\n",
      "       [ 4999.7693673 , 16004.57621628],\n",
      "       [ 1278.28594541, 12361.23023817],\n",
      "       [ 2742.77074584, 13903.30543774],\n",
      "       [ 4443.16520294, 15680.75458064],\n",
      "       [  905.784244  , 12219.90353958],\n",
      "       [ 4051.24158017, 15441.37660341],\n",
      "       [ 6025.15629794, 17490.80308564],\n",
      "       [-1659.51353128,  9881.15095016],\n",
      "       [ 1951.40248197, 13566.6001594 ],\n",
      "       [11065.38047005, 22754.63611353],\n",
      "       [  444.26622197, 17312.10294519],\n",
      "       [ 4638.03602195, 21709.28194521],\n",
      "       [ 4646.57197743, 21918.83178973],\n",
      "       [  864.80664907, 18335.76771808],\n",
      "       [ 2269.85332238, 19937.28104477],\n",
      "       [ 3911.62222865, 21773.35573851],\n",
      "       [  316.3988833 , 18370.34708386],\n",
      "       [ 3404.76923953, 21648.90712762],\n",
      "       [ 5322.32615558, 23754.69141158],\n",
      "       [-2417.99710835, 16200.69271081],\n",
      "       [ 1137.94634072, 19941.11448423],\n",
      "       [10197.6103602 , 29183.46440695]]))\n",
      "Inspecting Art ...\n",
      "Training 45 actual records ...\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,1,0)(0,1,0)[12]             : AIC=499.949, Time=0.04 sec\n",
      " ARIMA(1,1,0)(1,1,0)[12]             : AIC=477.330, Time=0.32 sec\n",
      " ARIMA(0,1,1)(0,1,1)[12]             : AIC=inf, Time=0.59 sec\n",
      " ARIMA(1,1,0)(0,1,0)[12]             : AIC=479.411, Time=0.09 sec\n",
      " ARIMA(1,1,0)(2,1,0)[12]             : AIC=477.186, Time=1.36 sec\n",
      " ARIMA(1,1,0)(2,1,1)[12]             : AIC=inf, Time=2.70 sec\n",
      " ARIMA(1,1,0)(1,1,1)[12]             : AIC=inf, Time=0.78 sec\n",
      " ARIMA(0,1,0)(2,1,0)[12]             : AIC=491.508, Time=0.24 sec\n",
      " ARIMA(2,1,0)(2,1,0)[12]             : AIC=478.930, Time=0.96 sec\n",
      " ARIMA(1,1,1)(2,1,0)[12]             : AIC=478.864, Time=1.08 sec\n",
      " ARIMA(0,1,1)(2,1,0)[12]             : AIC=481.094, Time=1.14 sec\n",
      " ARIMA(2,1,1)(2,1,0)[12]             : AIC=480.787, Time=2.66 sec\n",
      " ARIMA(1,1,0)(2,1,0)[12] intercept   : AIC=478.206, Time=1.70 sec\n",
      "\n",
      "Best model:  ARIMA(1,1,0)(2,1,0)[12]          \n",
      "Total fit time: 13.700 seconds\n",
      "Applying anomaly filter...\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,1,0)(0,1,0)[12]             : AIC=310.288, Time=0.06 sec\n",
      " ARIMA(1,1,0)(1,1,0)[12]             : AIC=291.529, Time=0.49 sec\n",
      " ARIMA(0,1,1)(0,1,1)[12]             : AIC=inf, Time=0.49 sec\n",
      " ARIMA(1,1,0)(0,1,0)[12]             : AIC=293.591, Time=0.11 sec\n",
      " ARIMA(1,1,0)(2,1,0)[12]             : AIC=inf, Time=2.72 sec\n",
      " ARIMA(1,1,0)(1,1,1)[12]             : AIC=inf, Time=0.91 sec\n",
      " ARIMA(1,1,0)(0,1,1)[12]             : AIC=inf, Time=0.51 sec\n",
      " ARIMA(1,1,0)(2,1,1)[12]             : AIC=inf, Time=1.07 sec\n",
      " ARIMA(0,1,0)(1,1,0)[12]             : AIC=305.730, Time=0.24 sec\n",
      " ARIMA(2,1,0)(1,1,0)[12]             : AIC=293.146, Time=0.40 sec\n",
      " ARIMA(1,1,1)(1,1,0)[12]             : AIC=inf, Time=0.44 sec\n",
      " ARIMA(0,1,1)(1,1,0)[12]             : AIC=inf, Time=0.29 sec\n",
      " ARIMA(2,1,1)(1,1,0)[12]             : AIC=inf, Time=0.77 sec\n",
      " ARIMA(1,1,0)(1,1,0)[12] intercept   : AIC=293.459, Time=0.81 sec\n",
      "\n",
      "Best model:  ARIMA(1,1,0)(1,1,0)[12]          \n",
      "Total fit time: 9.362 seconds\n",
      "Training 45 adjusted actual records ...\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,1,0)(0,1,0)[12]             : AIC=496.378, Time=0.02 sec\n",
      " ARIMA(1,1,0)(1,1,0)[12]             : AIC=478.057, Time=0.34 sec\n",
      " ARIMA(0,1,1)(0,1,1)[12]             : AIC=472.572, Time=0.51 sec\n",
      " ARIMA(0,1,1)(0,1,0)[12]             : AIC=476.368, Time=0.07 sec\n",
      " ARIMA(0,1,1)(1,1,1)[12]             : AIC=474.536, Time=1.15 sec\n",
      " ARIMA(0,1,1)(0,1,2)[12]             : AIC=474.534, Time=1.92 sec\n",
      " ARIMA(0,1,1)(1,1,0)[12]             : AIC=473.214, Time=0.49 sec\n",
      " ARIMA(0,1,1)(1,1,2)[12]             : AIC=inf, Time=2.53 sec\n",
      " ARIMA(0,1,0)(0,1,1)[12]             : AIC=inf, Time=0.39 sec\n",
      " ARIMA(1,1,1)(0,1,1)[12]             : AIC=473.202, Time=0.91 sec\n",
      " ARIMA(0,1,2)(0,1,1)[12]             : AIC=473.443, Time=0.97 sec\n",
      " ARIMA(1,1,0)(0,1,1)[12]             : AIC=inf, Time=0.53 sec\n",
      " ARIMA(1,1,2)(0,1,1)[12]             : AIC=inf, Time=1.64 sec\n",
      " ARIMA(0,1,1)(0,1,1)[12] intercept   : AIC=inf, Time=0.69 sec\n",
      "\n",
      "Best model:  ARIMA(0,1,1)(0,1,1)[12]          \n",
      "Total fit time: 12.187 seconds\n",
      "(array([1564.32452215, 1813.89013274, 2057.40358603, 1424.88330227,\n",
      "       1556.28543018, 1488.28376142, 1853.58676733, 1786.24779839,\n",
      "       1940.79385152, 1872.40344133, 1827.23014775, 2322.6485038 ,\n",
      "       1812.55280448, 2326.7609119 , 2284.52817541, 1780.24077251,\n",
      "       1888.03625397, 2061.69432294, 2082.0914064 , 2157.78497048,\n",
      "       2153.58751775, 2338.25807045, 2140.63638055, 2569.30833368]), array([[ 907.2394831 , 2221.4095612 ],\n",
      "       [1119.42161174, 2508.35865375],\n",
      "       [1196.25864116, 2918.5485309 ],\n",
      "       [ 505.47958118, 2344.28702336],\n",
      "       [ 534.76956026, 2577.8013001 ],\n",
      "       [ 403.78888663, 2572.77863621],\n",
      "       [ 691.51191455, 3015.66162011],\n",
      "       [ 562.86614362, 3009.62945316],\n",
      "       [ 651.96439472, 3229.62330832],\n",
      "       [ 525.76934313, 3219.03753952],\n",
      "       [ 422.36040213, 3232.09989336],\n",
      "       [ 863.64923238, 3781.64777522],\n",
      "       [ 224.10618874, 3400.99942023],\n",
      "       [ 669.13041819, 3984.39140562],\n",
      "       [ 529.89166883, 4039.16468199],\n",
      "       [ -46.61969431, 3607.10123934],\n",
      "       [ -20.54449361, 3796.61700156],\n",
      "       [  82.58980141, 4040.79884448],\n",
      "       [  29.94771001, 4134.23510279],\n",
      "       [  38.2758315 , 4277.29410947],\n",
      "       [ -33.22161275, 4340.39664825],\n",
      "       [  87.44686228, 4589.06927861],\n",
      "       [-173.22968779, 4454.50244889],\n",
      "       [ 194.58877781, 4944.02788955]]))\n",
      "(array([ 685.18460236, 1082.01990082, 1128.86397092,  644.77082259,\n",
      "        685.31299325,  805.28984637, 1000.9156921 ,  998.50981086,\n",
      "        587.81453883,  697.10556801,  663.67092598, 1374.77731683,\n",
      "        804.44995073, 1201.28524919, 1248.12931929,  764.03617096,\n",
      "        804.57834162,  924.55519474, 1120.18104048, 1117.77515923,\n",
      "        707.0798872 ,  816.37091639,  782.93627435, 1494.04266521]), array([[  48.55877245, 1321.81043227],\n",
      "       [ 443.14655482, 1720.89324681],\n",
      "       [ 488.56398689, 1769.16395494],\n",
      "       [   8.41644408, 1281.1252011 ],\n",
      "       [  46.93027762, 1323.69570887],\n",
      "       [ 164.98288217, 1445.59681057],\n",
      "       [ 358.69830416, 1643.13308005],\n",
      "       [ 354.38829021, 1642.63133151],\n",
      "       [ -58.20552241, 1233.83460006],\n",
      "       [  49.19164757, 1345.01948846],\n",
      "       [  13.85734736, 1313.4845046 ],\n",
      "       [ 722.92325998, 2026.63137368],\n",
      "       [ 110.26020419, 1498.63969728],\n",
      "       [ 503.82241091, 1898.74808748],\n",
      "       [ 547.84930423, 1948.40933435],\n",
      "       [  62.76797725, 1465.30436467],\n",
      "       [ 100.49326911, 1508.66341414],\n",
      "       [ 217.80178911, 1631.30860038],\n",
      "       [ 410.78043421, 1829.58164675],\n",
      "       [ 405.73805317, 1829.8122653 ],\n",
      "       [  -7.5840097 , 1421.7437841 ],\n",
      "       [  99.08877158, 1533.65306119],\n",
      "       [  63.03158974, 1502.84095897],\n",
      "       [ 771.34704649, 2216.73828393]]))\n"
     ]
    }
   ],
   "source": [
    "tsfm = TSFM(df=df, n_pred_period=24, date_variable='trsdate', target_variable='pnme', value_variable='value2', stop_date=\"2019-09-01\", section_list=[\"Accessories\", \"Art\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Applying anomaly filter...\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,1,0)(0,1,0)[12]             : AIC=381.678, Time=0.03 sec\n",
      " ARIMA(1,1,0)(1,1,0)[12]             : AIC=379.654, Time=0.24 sec\n",
      " ARIMA(0,1,1)(0,1,1)[12]             : AIC=inf, Time=0.29 sec\n",
      " ARIMA(1,1,0)(0,1,0)[12]             : AIC=377.660, Time=0.03 sec\n",
      " ARIMA(1,1,0)(0,1,1)[12]             : AIC=379.652, Time=0.09 sec\n",
      " ARIMA(1,1,0)(1,1,1)[12]             : AIC=381.531, Time=0.36 sec\n",
      " ARIMA(2,1,0)(0,1,0)[12]             : AIC=372.075, Time=0.05 sec\n",
      " ARIMA(2,1,0)(1,1,0)[12]             : AIC=373.961, Time=0.25 sec\n",
      " ARIMA(2,1,0)(0,1,1)[12]             : AIC=373.953, Time=0.15 sec\n",
      " ARIMA(2,1,0)(1,1,1)[12]             : AIC=375.942, Time=0.45 sec\n",
      " ARIMA(3,1,0)(0,1,0)[12]             : AIC=374.153, Time=0.10 sec\n",
      " ARIMA(2,1,1)(0,1,0)[12]             : AIC=374.296, Time=0.19 sec\n",
      " ARIMA(1,1,1)(0,1,0)[12]             : AIC=inf, Time=0.19 sec\n",
      " ARIMA(3,1,1)(0,1,0)[12]             : AIC=inf, Time=0.33 sec\n",
      " ARIMA(2,1,0)(0,1,0)[12] intercept   : AIC=374.104, Time=0.08 sec\n",
      "\n",
      "Best model:  ARIMA(2,1,0)(0,1,0)[12]          \n",
      "Total fit time: 2.848 seconds\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  value2\n",
       "trsdate                 \n",
       "2017-01-01   1231.950500\n",
       "2017-02-01    763.125500\n",
       "2017-03-01    909.847300\n",
       "2017-04-01   2418.690500\n",
       "2017-05-01    660.715500\n",
       "2017-06-01    633.688000\n",
       "2017-07-01   3163.397400\n",
       "2017-08-01   2839.981300\n",
       "2017-09-01   1324.108100\n",
       "2017-10-01   2472.099500\n",
       "2017-11-01   2500.555800\n",
       "2017-12-01   4930.503800\n",
       "2018-01-01    388.344800\n",
       "2018-02-01   1377.581700\n",
       "2018-03-01   1234.401500\n",
       "2018-04-01   1471.487300\n",
       "2018-05-01   1138.153200\n",
       "2018-06-01   1667.403900\n",
       "2018-07-01   4963.731000\n",
       "2018-08-01   1113.850300\n",
       "2018-09-01   5536.826100\n",
       "2018-10-01   4043.076500\n",
       "2018-11-01   4154.826900\n",
       "2018-12-01  11322.974000\n",
       "2019-01-01   1924.451100\n",
       "2019-02-01    342.978500\n",
       "2019-03-01   2378.950100\n",
       "2019-04-01   1535.633200\n",
       "2019-05-01   3770.316300\n",
       "2019-06-01   1584.778600\n",
       "2019-07-01   1114.469700\n",
       "2019-08-01   4419.863600\n",
       "2019-09-01   6335.046500\n",
       "2019-10-01   3317.126400\n",
       "2019-11-01   7612.600800\n",
       "2019-12-01   7721.643700\n",
       "2020-01-01   4039.229000\n",
       "2020-02-01   5542.509000\n",
       "2020-03-01   7281.430800\n",
       "2020-04-01   3782.314800\n",
       "2020-05-01   6965.780000\n",
       "2020-06-01   8977.450600\n",
       "2020-07-01   1330.289618\n",
       "2020-08-01   4978.472229\n",
       "2020-09-01  14129.479200"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>value2</th>\n    </tr>\n    <tr>\n      <th>trsdate</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2017-01-01</th>\n      <td>1231.950500</td>\n    </tr>\n    <tr>\n      <th>2017-02-01</th>\n      <td>763.125500</td>\n    </tr>\n    <tr>\n      <th>2017-03-01</th>\n      <td>909.847300</td>\n    </tr>\n    <tr>\n      <th>2017-04-01</th>\n      <td>2418.690500</td>\n    </tr>\n    <tr>\n      <th>2017-05-01</th>\n      <td>660.715500</td>\n    </tr>\n    <tr>\n      <th>2017-06-01</th>\n      <td>633.688000</td>\n    </tr>\n    <tr>\n      <th>2017-07-01</th>\n      <td>3163.397400</td>\n    </tr>\n    <tr>\n      <th>2017-08-01</th>\n      <td>2839.981300</td>\n    </tr>\n    <tr>\n      <th>2017-09-01</th>\n      <td>1324.108100</td>\n    </tr>\n    <tr>\n      <th>2017-10-01</th>\n      <td>2472.099500</td>\n    </tr>\n    <tr>\n      <th>2017-11-01</th>\n      <td>2500.555800</td>\n    </tr>\n    <tr>\n      <th>2017-12-01</th>\n      <td>4930.503800</td>\n    </tr>\n    <tr>\n      <th>2018-01-01</th>\n      <td>388.344800</td>\n    </tr>\n    <tr>\n      <th>2018-02-01</th>\n      <td>1377.581700</td>\n    </tr>\n    <tr>\n      <th>2018-03-01</th>\n      <td>1234.401500</td>\n    </tr>\n    <tr>\n      <th>2018-04-01</th>\n      <td>1471.487300</td>\n    </tr>\n    <tr>\n      <th>2018-05-01</th>\n      <td>1138.153200</td>\n    </tr>\n    <tr>\n      <th>2018-06-01</th>\n      <td>1667.403900</td>\n    </tr>\n    <tr>\n      <th>2018-07-01</th>\n      <td>4963.731000</td>\n    </tr>\n    <tr>\n      <th>2018-08-01</th>\n      <td>1113.850300</td>\n    </tr>\n    <tr>\n      <th>2018-09-01</th>\n      <td>5536.826100</td>\n    </tr>\n    <tr>\n      <th>2018-10-01</th>\n      <td>4043.076500</td>\n    </tr>\n    <tr>\n      <th>2018-11-01</th>\n      <td>4154.826900</td>\n    </tr>\n    <tr>\n      <th>2018-12-01</th>\n      <td>11322.974000</td>\n    </tr>\n    <tr>\n      <th>2019-01-01</th>\n      <td>1924.451100</td>\n    </tr>\n    <tr>\n      <th>2019-02-01</th>\n      <td>342.978500</td>\n    </tr>\n    <tr>\n      <th>2019-03-01</th>\n      <td>2378.950100</td>\n    </tr>\n    <tr>\n      <th>2019-04-01</th>\n      <td>1535.633200</td>\n    </tr>\n    <tr>\n      <th>2019-05-01</th>\n      <td>3770.316300</td>\n    </tr>\n    <tr>\n      <th>2019-06-01</th>\n      <td>1584.778600</td>\n    </tr>\n    <tr>\n      <th>2019-07-01</th>\n      <td>1114.469700</td>\n    </tr>\n    <tr>\n      <th>2019-08-01</th>\n      <td>4419.863600</td>\n    </tr>\n    <tr>\n      <th>2019-09-01</th>\n      <td>6335.046500</td>\n    </tr>\n    <tr>\n      <th>2019-10-01</th>\n      <td>3317.126400</td>\n    </tr>\n    <tr>\n      <th>2019-11-01</th>\n      <td>7612.600800</td>\n    </tr>\n    <tr>\n      <th>2019-12-01</th>\n      <td>7721.643700</td>\n    </tr>\n    <tr>\n      <th>2020-01-01</th>\n      <td>4039.229000</td>\n    </tr>\n    <tr>\n      <th>2020-02-01</th>\n      <td>5542.509000</td>\n    </tr>\n    <tr>\n      <th>2020-03-01</th>\n      <td>7281.430800</td>\n    </tr>\n    <tr>\n      <th>2020-04-01</th>\n      <td>3782.314800</td>\n    </tr>\n    <tr>\n      <th>2020-05-01</th>\n      <td>6965.780000</td>\n    </tr>\n    <tr>\n      <th>2020-06-01</th>\n      <td>8977.450600</td>\n    </tr>\n    <tr>\n      <th>2020-07-01</th>\n      <td>1330.289618</td>\n    </tr>\n    <tr>\n      <th>2020-08-01</th>\n      <td>4978.472229</td>\n    </tr>\n    <tr>\n      <th>2020-09-01</th>\n      <td>14129.479200</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "tsfm.get_actual_data(section=\"Accessories\", is_adjusted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Applying anomaly filter...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-ffbcba384dd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtsfm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accessories\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-a176529daf35>\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, section)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msection\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0mactual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_actual_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_adjusted\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0madjusted_actual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf_int_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manomaly_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_conf_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[1;31m# pred, ci = self.get_pred_data(section, return_conf_int=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-a176529daf35>\u001b[0m in \u001b[0;36manomaly_filter\u001b[1;34m(self, df, return_conf_int, n_rolling_period, alpha)\u001b[0m\n\u001b[0;32m    256\u001b[0m             ic_df = pd.DataFrame(\n\u001b[0;32m    257\u001b[0m                 data={\n\u001b[1;32m--> 258\u001b[1;33m                     \u001b[1;34m'lower'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreturning_ic_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m                     \u001b[1;34m'upper'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreturning_ic_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m                 },\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "tsfm.plot(\"Accessories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          id clientid     pid         pnme        value    trsdate  \\\n",
       "9     550984  test001  TEC-AC  Accessories  2472.099500 2017-10-01   \n",
       "10    551007  test001  TEC-AC  Accessories  2500.555800 2017-11-01   \n",
       "11    551361  test001  TEC-AC  Accessories  4930.503800 2017-12-01   \n",
       "12    550905  test001  TEC-AC  Accessories   388.344800 2018-01-01   \n",
       "13    550876  test001  TEC-AC  Accessories  1377.581700 2018-02-01   \n",
       "...      ...      ...     ...          ...          ...        ...   \n",
       "1097  550890  test001  FUR-TA       Tables  3299.692475 2018-10-01   \n",
       "1098  551190  test001  FUR-TA       Tables  6443.898925 2018-11-01   \n",
       "1099  550915  test001  FUR-TA       Tables  5426.960500 2018-12-01   \n",
       "1100  550912  test001  FUR-TA       Tables  2578.745550 2019-01-01   \n",
       "1101  551142  test001  FUR-TA       Tables  1847.513450 2019-02-01   \n",
       "\n",
       "           value2  \n",
       "9     2472.099500  \n",
       "10    2500.555800  \n",
       "11    4930.503800  \n",
       "12     388.344800  \n",
       "13    1377.581700  \n",
       "...           ...  \n",
       "1097  3299.692475  \n",
       "1098  6443.898925  \n",
       "1099  5426.960500  \n",
       "1100  2578.745550  \n",
       "1101  1847.513450  \n",
       "\n",
       "[163 rows x 7 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>clientid</th>\n      <th>pid</th>\n      <th>pnme</th>\n      <th>value</th>\n      <th>trsdate</th>\n      <th>value2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9</th>\n      <td>550984</td>\n      <td>test001</td>\n      <td>TEC-AC</td>\n      <td>Accessories</td>\n      <td>2472.099500</td>\n      <td>2017-10-01</td>\n      <td>2472.099500</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>551007</td>\n      <td>test001</td>\n      <td>TEC-AC</td>\n      <td>Accessories</td>\n      <td>2500.555800</td>\n      <td>2017-11-01</td>\n      <td>2500.555800</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>551361</td>\n      <td>test001</td>\n      <td>TEC-AC</td>\n      <td>Accessories</td>\n      <td>4930.503800</td>\n      <td>2017-12-01</td>\n      <td>4930.503800</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>550905</td>\n      <td>test001</td>\n      <td>TEC-AC</td>\n      <td>Accessories</td>\n      <td>388.344800</td>\n      <td>2018-01-01</td>\n      <td>388.344800</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>550876</td>\n      <td>test001</td>\n      <td>TEC-AC</td>\n      <td>Accessories</td>\n      <td>1377.581700</td>\n      <td>2018-02-01</td>\n      <td>1377.581700</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1097</th>\n      <td>550890</td>\n      <td>test001</td>\n      <td>FUR-TA</td>\n      <td>Tables</td>\n      <td>3299.692475</td>\n      <td>2018-10-01</td>\n      <td>3299.692475</td>\n    </tr>\n    <tr>\n      <th>1098</th>\n      <td>551190</td>\n      <td>test001</td>\n      <td>FUR-TA</td>\n      <td>Tables</td>\n      <td>6443.898925</td>\n      <td>2018-11-01</td>\n      <td>6443.898925</td>\n    </tr>\n    <tr>\n      <th>1099</th>\n      <td>550915</td>\n      <td>test001</td>\n      <td>FUR-TA</td>\n      <td>Tables</td>\n      <td>5426.960500</td>\n      <td>2018-12-01</td>\n      <td>5426.960500</td>\n    </tr>\n    <tr>\n      <th>1100</th>\n      <td>550912</td>\n      <td>test001</td>\n      <td>FUR-TA</td>\n      <td>Tables</td>\n      <td>2578.745550</td>\n      <td>2019-01-01</td>\n      <td>2578.745550</td>\n    </tr>\n    <tr>\n      <th>1101</th>\n      <td>551142</td>\n      <td>test001</td>\n      <td>FUR-TA</td>\n      <td>Tables</td>\n      <td>1847.513450</td>\n      <td>2019-02-01</td>\n      <td>1847.513450</td>\n    </tr>\n  </tbody>\n</table>\n<p>163 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df.query(\"trsdate > '2017-03-01' & trsdate < '2019-06-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}